---
sidebar_position: 3
sidebar_label: 安装高可用
---

## 概述

## 安装步骤

### 解压安装包

```
tar -zxvf hadoop-3.2.3.tar.gz -C ../module/
```

```
<!-- 同步安装包 -->

/home/bigdata/shell/xsync.sh /home/bigdata/module/hadoop-3.2.3
```

### 配置环境变量


```
sudo vim /etc/profile.d/my_env.sh

#HADOOP_HOME

export HADOOP_HOME=/home/bigdata/module/hadoop-3.2.3
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

```
同步环境变量文件。

```
/home/bigdata/shell/xsync.sh /etc/profile.d/my_env.sh
```

```
source /etc/profile.d/my_env.sh
```

## Hadoop 相关文件说明

- bin目录：存放对Hadoop相关服务（hdfs，yarn，mapred）进行操作的脚本

- etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件

- lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能）

- sbin目录：存放启动或停止Hadoop相关服务的脚本

- share目录：存放Hadoop的依赖jar包、文档、和官方案例

## 修改配置文件

### workers

workers 文件配置是在集群脚本启动datanode和nodemanage包含的机器，**不包含namenode和resourcemanager**。

由于规划

| 主机          | 规划设置主机名 | 角色                                            |
|--------------|-----------------  |-------------------------------------------------|
| 10.240.8.68  | master1           | NameNode、DataNode、ResourceManager、NodeManager |
| 10.240.8.229 | master2           | SecondaryNameNode、DataNode、NodeManager        |
| 10.240.8.185 | node1             | DataNode、NodeManager                           |

master1，master2，node1 都有DataNode、NodeManager，所以修改如下。

```
vi /home/bigdata/module/hadoop-3.2.3/etc/hadoop/workers

master1
master2
node1
```

### core-site.xml

准备好数据盘,然后把数据盘的所有权给bigdata

```
sudo chown bigdata:bigdata -R /datadrive/
sudo chmod 744 /datadrive
```

```
vi /home/bigdata/module/hadoop-3.2.3/etc/hadoop/core-site.xml
```

```
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>

    <property>
        <!--指定 namenode 的 hdfs 协议文件系统的通信地址-->
        <name>fs.defaultFS</name>
		<!--指定hdfs高可用的集群名称-->
        <value>hdfs://bigdatacluster</value>
    </property>
    <property>
        <!--指定 hadoop 集群存储临时文件的目录-->
        <name>hadoop.tmp.dir</name>
        <value>/datadrive</value>
    </property>
	
	<!-- 配置HDFS网页登录使用的静态用户为bigdata -->
  <property>
    <name>hadoop.http.staticuser.user</name>
    <value>bigdata</value>
  </property>
 
 <!-- 回收站 -->
    <!-- 表示回收站文件在删除之前将被保留 3600 秒 -->
    <property>
        <name>fs.trash.interval</name>
        <value>3600</value>
    </property>
 
 <!-- 生成 Trash 检查点的时间间隔。检查点包含用户删除的文件和目录的快照。 -->
    <property>
        <name>fs.trash.checkpoint.interval</name>
        <value>3600</value>
    </property>
 
    <!-- 配置该bigdata(superUser)允许通过代理访问的主机节点 -->
    <property>
        <name>hadoop.proxyuser.bigdata.hosts</name>
        <value>*</value>
    </property>
    <!-- 配置该bigdata(superUser)允许通过代理用户所属组 -->
    <property>
        <name>hadoop.proxyuser.bigdata.groups</name>
        <value>*</value>
    </property>
    <!-- 配置该bigdata(superUser)允许通过代理的用户-->
    <property>
        <name>hadoop.proxyuser.bigdata.users</name>
        <value>*</value>
    </property>
 
 <!-- 指定zkfc要连接的zkServer地址 -->
<property>
	<name>ha.zookeeper.quorum</name>
	<value>master1:2181,master2:2181,node1:2181</value>
</property>

<!-- Hue  -->
<property>
    <name>hadoop.proxyuser.hdfs.hosts</name>
    <value>*</value>
</property>
<property>
    <name>hadoop.proxyuser.hdfs.groups</name>
    <value>*</value>
</property>

<property>
  <name>hadoop.proxyuser.httpfs.hosts</name>
  <value>*</value>
</property>
<property>
  <name>hadoop.proxyuser.httpfs.groups</name>
  <value>*</value>
</property>

<property>
    	<name>hadoop.proxyuser.hue.hosts</name>
    	<value>*</value>
    </property>
    <property>
    	<name>hadoop.proxyuser.hue.groups</name>
    	<value>*</value>
    </property>

</configuration>

```

### hdfs-site.xml

```
vi /home/bigdata/module/hadoop-3.2.3/etc/hadoop/hdfs-site.xml
```
```
touch /home/bigdata/module/hadoop-3.2.3/etc/blacklist 
```

```
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>

<!-- NameNode数据存储目录 -->
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file://${hadoop.tmp.dir}/name</value>
  </property>
<!-- DataNode数据存储目录 -->
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file://${hadoop.tmp.dir}/data</value>
  </property>
<!-- JournalNode数据存储目录 -->
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>${hadoop.tmp.dir}/jn</value>
  </property>
<!-- 完全分布式集群名称 对应core.xml里面的fs.defaultFS-->
  <property>
    <name>dfs.nameservices</name>
    <value>bigdatacluster</value>
  </property>
<!-- 集群中NameNode节点都有哪些 -->
  <property>
    <name>dfs.ha.namenodes.bigdatacluster</name>
    <value>nn1,nn2</value>
  </property>
<!-- NameNode的RPC通信地址 -->
  <property>
    <name>dfs.namenode.rpc-address.bigdatacluster.nn1</name>
    <value>master1:8020</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.bigdatacluster.nn2</name>
    <value>master2:8020</value>
  </property>
<!-- NameNode的http通信地址 -->
  <property>
    <name>dfs.namenode.http-address.bigdatacluster.nn1</name>
    <value>master1:9870</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.bigdatacluster.nn2</name>
    <value>master2:9870</value>
  </property>
<!-- 指定NameNode元数据在JournalNode上的存放位置 -->
  <property>
<name>dfs.namenode.shared.edits.dir</name>
<value>qjournal://master1:8485;master2:8485;node1:8485/bigdatacluster</value>
  </property>
<!-- 访问代理类：client用于确定哪个NameNode为Active -->
  <property>
    <name>dfs.client.failover.proxy.provider.bigdatacluster</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
<!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 -->
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
  </property>
<!-- 使用隔离机制时需要ssh秘钥登录-->
  <property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>/home/bigdata/.ssh/id_rsa</value>
  </property>
 
  <!-- 配置黑名单 -->
     <property>
        <name>dfs.hosts.exclude</name>
        <value>/home/bigdata/module/hadoop-3.1.3/etc/blacklist</value>
    </property>
	
	<!-- 启用nn故障自动转移 -->
<property>
	<name>dfs.ha.automatic-failover.enabled</name>
	<value>true</value>
</property>

<!-- HUE -->
<property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
</property>
<property>
    <name>dfs.permissions.enabled</name>
    <value>false</value>
</property>

<!-- 磁盘扩容策略 -->
<!-- 设置数据存储策略，默认为轮询，现在的情况显然应该用“选择空间多的磁盘存”模式 -->
<property>
    <name>dfs.datanode.fsdataset.volume.choosing.policy</name>
    <value>org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy</value>
</property>
<!-- 默认值0.75。它的含义是数据块存储到可用空间多的卷上的概率，由此可见，这个值如果取0.5以下，对该策略而言是毫无意义的，一般就采用默认值。-->
<property>
    <name>dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction</name>
    <value>0.9f</value>
</property>
<!-- 配置各个磁盘的均衡阈值的，默认为10G（10737418240），在此节点的所有数据存储的目录中，找一个占用最大的，找一个占用最小的，如果在两者之差在10G的范围内，那么块分配的方式是轮询。 -->
<property>
  <name>dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold</name>         
  <value>10737418240</value>
</property>

<property>
  <name>dfs.disk.balancer.enabled</name>
  <value>true</value>
</property>
</configuration>

```

### yarn-site.xml

```
vi /home/bigdata/module/hadoop-3.2.3/etc/hadoop/yarn-site.xml
```

```
<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<configuration>
 
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
 
    <!-- 启用resourcemanager ha -->
    <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property>
 
    <!-- 声明两台resourcemanager的地址 -->
    <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>cluster-yarn1</value>
    </property>
    <!--指定resourcemanager的逻辑列表-->
    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
</property>
<!-- ========== rm1的配置 ========== -->
<!-- 指定rm1的主机名 -->
    <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>master1</value>
</property>
<!-- 指定rm1的web端地址 -->
<property>
     <name>yarn.resourcemanager.webapp.address.rm1</name>
     <value>master1:8088</value>
</property>
<!-- 指定rm1的内部通信地址 -->
<property>
     <name>yarn.resourcemanager.address.rm1</name>
     <value>master1:8032</value>
</property>
<!-- 指定AM向rm1申请资源的地址 -->
<property>
     <name>yarn.resourcemanager.scheduler.address.rm1</name>  
     <value>master1:8030</value>
</property>
<!-- 指定供NM连接的地址 -->  
<property>
     <name>yarn.resourcemanager.resource-tracker.address.rm1</name>
     <value>master1:8031</value>
</property>
<!-- ========== rm2的配置 ========== -->
    <!-- 指定rm2的主机名 -->
    <property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>master2</value>
</property>
<property>
     <name>yarn.resourcemanager.webapp.address.rm2</name>
     <value>master2:8088</value>
</property>
<property>
     <name>yarn.resourcemanager.address.rm2</name>
     <value>master2:8032</value>
</property>
<property>
     <name>yarn.resourcemanager.scheduler.address.rm2</name>
     <value>master2:8030</value>
</property>
<property>
     <name>yarn.resourcemanager.resource-tracker.address.rm2</name>
     <value>master2:8031</value>
</property>
 
    <!-- 指定zookeeper集群的地址 --> 
    <property>
        <name>yarn.resourcemanager.zk-address</name>
        <value>master1:2181,master2:2181,node1:2181</value>
    </property>
 
    <!-- 启用自动恢复 --> 
    <property>
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
    </property>
 
    <!-- 指定resourcemanager的状态信息存储在zookeeper集群 --> 
    <property>
        <name>yarn.resourcemanager.store.class</name>     
		<value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
</property>
<!-- 环境变量的继承 -->
 <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
 
	 <!-- 开启日志聚集功能 -->
  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>
  <!-- 设置日志聚集服务器地址 -->
 <!-- 设置日志聚集服务器地址 -->
  <property>  
    <name>yarn.log.server.url</name>  
    <value>http://master1:19888/jobhistory/logs</value>
  </property>
  <!-- 设置日志保留时间为7天 -->
  <property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>604800</value>
  </property>
 
    <!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
 
    <!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>

<!--nodemanager 的内存-->
    <property>
      <name>yarn.nodemanager.resource.memory-mb</name>
      <value>51200</value>
    </property>
<!--缺少该项容易引发异常：Invalid event: APP_UPDATE_SAVED at ACCEPTED-->
	<property>
        <name>yarn.app.mapreduce.am.resource.mb</name>
        <value>2048</value>
    </property>
    <property>
	<!--表示该节点服务器上yarn可以使用的虚拟CPU个数，可以设置为实际机器的两倍-->
      <name>yarn.nodemanager.resource.cpu-vcores</name>
      <value>28</value>
    </property>

</configuration>

```

### mapred-site.xml

```
vi /home/bigdata/module/hadoop-3.2.3/etc/hadoop/mapred-site.xml
```

```
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>

<!-- 启用jvm重用 -->
<property>
    <name>mapreduce.job.jvm.numtasks</name>
    <value>10</value>
    <description>How many tasks to run per jvm,if set to -1 ,there is  no limit</description>
</property>

<!--    
 <property>
      <name>mapreduce.job.tracker</name>
      <value>hdfs://master1:8001</value>
      <final>true</final>
 </property>
-->
<property>
        <!--指定 mapreduce 作业运行在 yarn 上-->
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
	
<property>
  <name>yarn.app.mapreduce.am.env</name>
  <value>HADOOP_MAPRED_HOME=/home/bigdata/module/hadoop-3.2.3</value>
</property>
<property>
  <name>mapreduce.map.env</name>
  <value>HADOOP_MAPRED_HOME=/home/bigdata/module/hadoop-3.2.3</value>
</property>
<property>
  <name>mapreduce.reduce.env</name>
  <value>HADOOP_MAPRED_HOME=/home/bigdata/module/hadoop-3.2.3</value>
</property>
 
<!-- 历史服务器端地址 -->
  <property>
    <name>mapreduce.jobhistory.address</name>
    <value>master1:10020</value>
  </property>
  <!-- 历史服务器web端地址 -->
  <property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>master1:19888</value>
  </property>

</configuration>

```

### capacity-scheduler.xml

```
vi /home/bigdata/module/hadoop-3.2.3/etc/hadoop/capacity-scheduler.xml
```

```
<!--
       Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>

<property>
    <name>yarn.scheduler.capacity.maximum-applications</name>
    <value>10000</value>
    <description>
      Maximum number of applications that can be pending and running.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
    <value>0.6</value>
    <description>
      Maximum percent of resources in the cluster which can be used to run 
      application masters i.e. controls number of concurrent running
      applications.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.resource-calculator</name>
    <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>
    <description>
      The ResourceCalculator implementation to be used to compare 
      Resources in the scheduler.
      The default i.e. DefaultResourceCalculator only uses Memory while
      DominantResourceCalculator uses dominant-resource to compare 
      multi-dimensional resources such as Memory, CPU etc.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.root.queues</name>
    <value>high,default</value>
    <description>
      The queues at the this level (root is the root queue).
    </description>
  </property>
<!--
            队列占比
-->
  <property>
    <name>yarn.scheduler.capacity.root.high.capacity</name>
    <value>60</value>
    <description>Default queue target capacity.</description>
  </property>

  
   <property>
    <name>yarn.scheduler.capacity.root.default.capacity</name>
    <value>40</value>
    <description>Default queue target capacity.</description>
  </property>

  
<!--
            百分比
-->
  <property>
    <name>yarn.scheduler.capacity.root.high.user-limit-factor</name>
    <value>1</value>
    <description>
      Default queue user limit a percentage from 0.0 to 1.0.
    </description>
  </property>


    <property>
    <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>
    <value>1</value>
    <description>
      Default queue user limit a percentage from 0.0 to 1.0.
    </description>
  </property>

<!--
            运行状态
-->
  <property>
    <name>yarn.scheduler.capacity.root.high.maximum-capacity</name>
    <value>100</value>
    <description>
      The maximum capacity of the default queue. 
    </description>
  </property>
  
    <property>
    <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
    <value>100</value>
    <description>
      The maximum capacity of the default queue. 
    </description>
  </property>


   <property>
    <name>yarn.scheduler.capacity.root.high.state</name>
    <value>RUNNING</value>
    <description>
      The state of the default queue. State can be one of RUNNING or STOPPED.
    </description>
  </property>
  
  
     <property>
    <name>yarn.scheduler.capacity.root.default.state</name>
    <value>RUNNING</value>
    <description>
      The state of the default queue. State can be one of RUNNING or STOPPED.
    </description>
  </property>

<!--
            权限
-->
  <property>
    <name>yarn.scheduler.capacity.root.high.acl_submit_applications</name>
    <value>*</value>
    <description>
      The ACL of who can submit jobs to the default queue.
    </description>
  </property>


    <property>
    <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>
    <value>*</value>
    <description>
      The ACL of who can submit jobs to the default queue.
    </description>
  </property>
<!--
            权限
-->
  <property>
    <name>yarn.scheduler.capacity.root.high.acl_administer_queue</name>
    <value>*</value>
    <description>
      The ACL of who can administer jobs on the default queue.
    </description>
  </property>

  
      <property>
    <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>
    <value>*</value>
    <description>
      The ACL of who can administer jobs on the default queue.
    </description>
  </property>

<!--
            权限
-->
  <property>
    <name>yarn.scheduler.capacity.root.high.acl_application_max_priority</name>
    <value>*</value>
    <description>
      The ACL of who can submit applications with configured priority.
      For e.g, [user={name} group={name} max_priority={priority} default_priority={priority}]
    </description>
  </property>

  
      <property>
    <name>yarn.scheduler.capacity.root.default.acl_application_max_priority</name>
    <value>*</value>
    <description>
      The ACL of who can submit applications with configured priority.
      For e.g, [user={name} group={name} max_priority={priority} default_priority={priority}]
    </description>
  </property>

<!--
            权限
-->
   <property>
     <name>yarn.scheduler.capacity.root.high.maximum-application-lifetime
     </name>
     <value>-1</value>
     <description>
        Maximum lifetime of an application which is submitted to a queue
        in seconds. Any value less than or equal to zero will be considered as
        disabled.
        This will be a hard time limit for all applications in this
        queue. If positive value is configured then any application submitted
        to this queue will be killed after exceeds the configured lifetime.
        User can also specify lifetime per application basis in
        application submission context. But user lifetime will be
        overridden if it exceeds queue maximum lifetime. It is point-in-time
        configuration.
        Note : Configuring too low value will result in killing application
        sooner. This feature is applicable only for leaf queue.
     </description>
   </property>



      <property>
     <name>yarn.scheduler.capacity.root.default.maximum-application-lifetime
     </name>
     <value>-1</value>
     <description>
        Maximum lifetime of an application which is submitted to a queue
        in seconds. Any value less than or equal to zero will be considered as
        disabled.
        This will be a hard time limit for all applications in this
        queue. If positive value is configured then any application submitted
        to this queue will be killed after exceeds the configured lifetime.
        User can also specify lifetime per application basis in
        application submission context. But user lifetime will be
        overridden if it exceeds queue maximum lifetime. It is point-in-time
        configuration.
        Note : Configuring too low value will result in killing application
        sooner. This feature is applicable only for leaf queue.
     </description>
   </property>

<!--
            生命周期
-->
   <property>
     <name>yarn.scheduler.capacity.root.high.default-application-lifetime
     </name>
     <value>-1</value>
     <description>
        Default lifetime of an application which is submitted to a queue
        in seconds. Any value less than or equal to zero will be considered as
        disabled.
        If the user has not submitted application with lifetime value then this
        value will be taken. It is point-in-time configuration.
        Note : Default lifetime can't exceed maximum lifetime. This feature is
        applicable only for leaf queue.
     </description>
   </property>
   
   <property>
        <name>yarn.scheduler.capacity.root.default.default-application-lifetime
     </name>
     <value>-1</value>
     <description>
        Default lifetime of an application which is submitted to a queue
        in seconds. Any value less than or equal to zero will be considered as
        disabled.
        If the user has not submitted application with lifetime value then this
        value will be taken. It is point-in-time configuration.
        Note : Default lifetime can't exceed maximum lifetime. This feature is
        applicable only for leaf queue.
     </description>
   </property>

  <property>
    <name>yarn.scheduler.capacity.node-locality-delay</name>
    <value>40</value>
    <description>
      Number of missed scheduling opportunities after which the CapacityScheduler 
      attempts to schedule rack-local containers.
      When setting this parameter, the size of the cluster should be taken into account.
      We use 40 as the default value, which is approximately the number of nodes in one rack.
      Note, if this value is -1, the locality constraint in the container request
      will be ignored, which disables the delay scheduling.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.rack-locality-additional-delay</name>
    <value>-1</value>
    <description>
      Number of additional missed scheduling opportunities over the node-locality-delay
      ones, after which the CapacityScheduler attempts to schedule off-switch containers,
      instead of rack-local ones.
      Example: with node-locality-delay=40 and rack-locality-delay=20, the scheduler will
      attempt rack-local assignments after 40 missed opportunities, and off-switch assignments
      after 40+20=60 missed opportunities.
      When setting this parameter, the size of the cluster should be taken into account.
      We use -1 as the default value, which disables this feature. In this case, the number
      of missed opportunities for assigning off-switch containers is calculated based on
      the number of containers and unique locations specified in the resource request,
      as well as the size of the cluster.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.queue-mappings</name>
    <value></value>
    <description>
      A list of mappings that will be used to assign jobs to queues
      The syntax for this list is [u|g]:[name]:[queue_name][,next mapping]*
      Typically this list will be used to map users to queues,
      for example, u:%user:%user maps all users to queues with the same name
      as the user.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.queue-mappings-override.enable</name>
    <value>false</value>
    <description>
      If a queue mapping is present, will it override the value specified
      by the user? This can be used by administrators to place jobs in queues
      that are different than the one specified by the user.
      The default is false.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.per-node-heartbeat.maximum-offswitch-assignments</name>
    <value>1</value>
    <description>
      Controls the number of OFF_SWITCH assignments allowed
      during a node's heartbeat. Increasing this value can improve
      scheduling rate for OFF_SWITCH containers. Lower values reduce
      "clumping" of applications on particular nodes. The default is 1.
      Legal values are 1-MAX_INT. This config is refreshable.
    </description>
  </property>


  <property>
    <name>yarn.scheduler.capacity.application.fail-fast</name>
    <value>false</value>
    <description>
      Whether RM should fail during recovery if previous applications'
      queue is no longer valid.
    </description>
  </property>

</configuration>

```

### hadoop-env.sh

```
mkdir /home/bigdata/module/hadoop-3.2.3/pid
```

```
vi /home/bigdata/module/hadoop-3.2.3/etc/hadoop/hadoop-env.sh
```

```
export HDFS_NAMENODE_USER=bigdata
export HADOOP_PID_DIR=/home/bigdata/module/hadoop-3.2.3/pid
```

### 同步配置文件

```
/home/bigdata/shell/xsync.sh /home/bigdata/module/hadoop-3.2.3/etc/hadoop
```

## 启动集群

### 启动准备

- 启动Zookeeper以后，然后再初始化HA在Zookeeper中状态(在对应的namenode上面其中一台格式化就行，最好这种情况是master1主节点)。

```
hdfs zkfc -formatZK
```

- 启动journalnode他是8485端口(在datanode的所有机器上面启动)。

```
hdfs --daemon start journalnode
```

- 然后格式化（namenode：主节点格式化，这里也就是master1）。

```
hdfs namenode -format
```

- master1启动namenode (master2先不用启动)。

```
hdfs --daemon start namenode
```

- master2然后同步namenode数据。

```
hdfs namenode -bootstrapStandby 
```

> 运行集群的时候注意：如果有JAVA_HOME找不到，那么就把hadoop里面的bin和sbin里面的执行脚本都加一个JAVA_HOME变量，比如：JAVA_HOME="/home/bigdata/module/jdk1.8.0_161"加入到脚本靠前的位置就行了。

### 集群启动脚本

```
vi /home/bigdata/shell/hadoop_server.sh

chmod 777 /home/bigdata/shell/hadoop_server.sh

/home/bigdata/shell/hadoop_server.sh start
```

```
#!/bin/bash
if [ $# -lt 1 ]
then
    echo "No Args Input..."
    exit ;
fi
case $1 in
"start")
        echo " =================== 启动 hadoop集群 ==================="
		echo " master1 的journalnode启动"
        ssh master1 "hdfs --daemon start journalnode"
        echo " master2 的journalnode启动"
        ssh master2 "hdfs --daemon start journalnode"
		echo " node1 的journalnode启动"
        ssh node1 "hdfs --daemon start journalnode"
		
		
        echo " --------------- 启动 hdfs ---------------"
        ssh master1 "/home/bigdata/module/hadoop-3.2.3/sbin/start-dfs.sh"
        echo " --------------- 启动 yarn ---------------"
        ssh master2 "/home/bigdata/module/hadoop-3.2.3/sbin/start-yarn.sh"
		
        echo " --------------- 启动 historyserver ---------------"
        ssh master1 "/home/bigdata/module/hadoop-3.2.3/bin/mapred --daemon start historyserver"  
        echo " --------------- 启动 httpfs ---------------"
        ssh master1 "/home/bigdata/module/hadoop-3.2.3/bin/hdfs --daemon start httpfs"
;;
"stop")
        echo " --------------- 关闭httpfs ---------------"
        ssh master1 "/home/bigdata/module/hadoop-3.2.3/bin/hdfs --daemon stop httpfs"
        echo " =================== 关闭 hadoop集群 ==================="
        echo " --------------- 关闭 historyserver ---------------"
        ssh master1 "/home/bigdata/module/hadoop-3.2.3/bin/mapred --daemon stop historyserver"
		
        echo " --------------- 关闭 yarn ---------------"
        ssh master2 "/home/bigdata/module/hadoop-3.2.3/sbin/stop-yarn.sh"
        echo " --------------- 关闭 hdfs ---------------"
        ssh master1 "/home/bigdata/module/hadoop-3.2.3/sbin/stop-dfs.sh"
		
		echo " master1 的journalnode关闭"
        ssh master1 "hdfs --daemon stop journalnode"
        echo " master2 的journalnode关闭"
        ssh master2 "hdfs --daemon stop journalnode"
		echo " node1 的journalnode关闭"
        ssh node1 "hdfs --daemon stop journalnode"
;;
*)
    echo "Input Args Error..."
;;
esac

```

## 测试看是否启动成功

### 查看hdfs的高可用状态
```
hdfs haadmin -getAllServiceState
```

```
yarn rmadmin -getAllServiceState
```

```
[bigdata@master1 shell]$ hdfs haadmin -getAllServiceState
master1:8020                                       standby   
master2:8020                                       active  

[bigdata@master1 shell]$ yarn rmadmin -getAllServiceState
master1:8033                                       standby   
master2:8033                                       active  
```
- 测试resourcemanger高可用是否可用。

```
/home/bigdata/module/hadoop-3.2.3/sbin/yarn-daemon.sh start resourcemanager
```

```
/home/bigdata/module/hadoop-3.2.3/sbin/yarn-daemon.sh stop resourcemanager
```

- 测试namenode高可用是否可用。

```
hdfs --daemon start namenode
```

```
hdfs --daemon stop namenode
```

### 提交一个 mr 程序。

```
hadoop dfs -mkdir /data

hadoop dfs -put /home/bigdata/shell/hadoop_server.sh /data
```

```
hadoop jar /home/bigdata/module/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar wordcount  -Dmapreduce.job.queuename=default /data /output
```

